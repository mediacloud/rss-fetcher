#!/bin/sh

# install rss-fetcher crontab
# must be run as root
# _can_ be run for user instances
INSTANCE=$1

if [ -z "$INSTANCE" ]; then
   echo Usage: $0 INSTANCE 1>&2
   exit 1
fi

SCRIPT_DIR=$(dirname $0)
COMMON_SH=$SCRIPT_DIR/common.sh
if [ ! -f $COMMON_SH ]; then
    echo cannot find $COMMON_SH 1>&2
    exit 1
fi
. $COMMON_SH

check_root

# filename must use letters and dashes only!!!:
CRONTAB=/etc/cron.d/$APP

# server log directory for crontab entries.
# files not rotated, so only save output from last invocation.
LOGDIR=/var/tmp

if [ "x$LOGDIR" != x/var/tmp ]; then
    # NOTE!!! LOGDIR for crontabs outside of app "storage" area; only saving output of last run
    # all scripts/*.py log to /app/storage/logs, w/ daily rotation and 7 day retention
    test -d $LOGDIR || mkdir -p $LOGDIR
fi

CRONTEMP="/tmp/$APP.cron.tmp"

# prevent world/group write to CRONTEMP/CRONTAB
umask 022

# run periodic scripts in fetcher container rather than firing up a
# new container just for the duration of the script.
DOKKU_RUN_PERIODIC="/usr/bin/dokku enter $APP fetcher"

cat >$CRONTEMP <<EOF
# MACHINE CREATED!!! PLEASE DO NOT EDIT THIS FILE!!!
# edit and run rss-fetcher/dokku-scripts/crontab.sh instead!!!!!!
#
# only saving output from last run; all apps log to /app/storage/logs
#
# generate RSS output files (try multiple times a day, in case of bad code, or downtime)
# NOTE! production instance runs "aws s3 sync" hourly lower down in crontab, and shouldn't
# run too soon after to allow generator to run to completion.
15 * * * * root $DOKKU_RUN_PERIODIC ./run-gen-daily-story-rss.sh > $LOGDIR/$APP-generator.log 2>&1
#
# archive old DB table entries (non-critical); production aws s3 sync should run after this
# (before 2am standard time rollback, in case we never get rid of time changes, and server not configured in UTC)
30 1 * * * root $DOKKU_RUN_PERIODIC python -m scripts.db_archive --verbose --delete > $LOGDIR/$APP-archiver.log 2>&1
EOF

# maybe do backups for staging as well (to fully test this script)?!
# would need separate buckets (and keys for those buckets),
# or at least a different prefix in the same bucket).

DB_BACKUP_POLICY=mediacloud-web-tools-db-backup-get-put-delete
DB_BACKUP_KEYNAME=mediacloud-webtools-db-backup

# change to true to enable backup of roll-over files:
# off 5/21/2024 -phil
BACKUP_DB_ARCHIVE=false

if [ "x$INSTANCE" = xprod ]; then
    if dpkg --list | grep awscli >/dev/null; then
	echo found awscli
    else
	echo installing awscli
	apt install -y awscli
    fi

    # use to run aws s3 sync for RSS files and DB backup
    # only requires that user be permenant, and have read access to $STDIR
    BACKUP_USER=root
    AWS_CREDS_DIR=$(eval echo ~$BACKUP_USER)/.aws
    AWS_CREDS=$AWS_CREDS_DIR/credentials

    # profiles (section) in $AWS_CREDS file
    DB_BACKUP_PROFILE=${APP}-backup
    DB_BACKUP_BUCKET=mediacloud-rss-fetcher-backup

    # disabled 2024-08-16
    # requires section in ~BACKUP_USER/.aws/credentials
    #S3_RSS_PROFILE=${APP}-rss
    #S3_RSS_BUCKET=mediacloud-public/backup-daily-rss

    # requires section in ~BACKUP_USER/.aws/credentials
    B2_RSS_PROFILE=b2-rss-fetcher-rss
    B2_RSS_BUCKET=mediacloud-public/daily-rss/rss-fetcher
    B2_RSS_REGION=us-east-005

    test -d $AWS_CREDS_DIR || mkdir $AWS_CREDS_DIR
    # check for profile in .aws/credentials file (or create bogus entry)
    check_aws_creds() {
	local PROFILE=$1
	local BOGUS=${PROFILE}-replace-me
	local POLICY=$2
	local KEYNAME=$3
	if [ -z "$PROFILE" ]; then
	    return
	fi
	if ! grep "\[$PROFILE\]" $AWS_CREDS >/dev/null 2>&1; then
	    (
		echo "[$PROFILE]"
		echo "aws_access_key_id = $BOGUS"
		echo "aws_secret_access_key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
	    ) >> $AWS_CREDS
	fi
	if grep $BOGUS $AWS_CREDS >/dev/null; then
	    echo '' 1>&2
	    echo "*** Need valid $PROFILE profile/section in $AWS_CREDS ***" 1>&2
	    if [ "x$POLICY" != x ]; then
		echo " (requires key with $POLICY policy attached (ie; $KEYNAME key)" 1>&2
	    fi
	fi
    }

    $BACKUP_DB_ARCHIVE && check_aws_creds $DB_BACKUP_PROFILE $DB_BACKUP_POLICY $DB_BACKUP_KEYNAME
    check_aws_creds "$S3_RSS_PROFILE" mediacloud-public-get-put-delete mediawords-public-s3
    check_aws_creds "$B2_RSS_PROFILE" '' mediacloud-public-rw
    chmod 600 $AWS_CREDS
    chown $BACKUP_USER $AWS_CREDS

    # copy generated RSS files to public S3 bucket, if enabled
    if [ -n "$S3_RSS_BUCKET" -a -n "$S3_RSS_PROFILE" ]; then
	echo "45 * * * * $BACKUP_USER aws s3 --profile $S3_RSS_PROFILE sync $STDIR/rss-output-files/ s3://$S3_RSS_BUCKET/ > $LOGDIR/rss-fetcher-aws-sync-rss-mc.log 2>&1" >> $CRONTEMP
    fi

    # copy generated RSS files to public B2 bucket using aws command
    echo "45 * * * * $BACKUP_USER aws s3 --profile $B2_RSS_PROFILE --endpoint https://s3.${B2_RSS_REGION}.backblazeb2.com sync $STDIR/rss-output-files/ s3://$B2_RSS_BUCKET > $LOGDIR/rss-fetcher-b2-sync-rss-mc.log 2>&1" >> $CRONTEMP

    # copy archived rows in CSV files to private bucket (NOTE! After "run archiver" entry created above)
    $BACKUP_DB_ARCHIVE && echo "45 1 * * * $BACKUP_USER aws s3 --profile $DB_BACKUP_PROFILE sync $STDIR/db-archive/ s3://$DB_BACKUP_BUCKET/ > $LOGDIR/rss-fetcher-aws-sync-dbarch-mc.log 2>&1" >> $CRONTEMP

    # sync feeds from mcweb (web-search server)
    echo "*/5 * * * * root $DOKKU_RUN_PERIODIC python -m scripts.update_feeds > $LOGDIR/rss-fetcher-update.log 2>&1" >> $CRONTEMP
fi

if [ -f $CRONTAB ]; then
    if cmp -s $CRONTAB $CRONTEMP; then
	echo no change to $CRONTAB
	rm -f $CRONTEMP
    else
	SAVE=/tmp/$APP.cron.$$
	echo $CRONTAB differs, saving as $SAVE
	mv $CRONTAB $SAVE
	echo updating $CRONTAB
	mv $CRONTEMP $CRONTAB
    fi
else
    echo installing $CRONTAB
    mv $CRONTEMP $CRONTAB
fi
